import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
import tensorflow as tf

# ğŸ“Œ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
data = pd.read_csv("data/macro_data_scaled.csv", parse_dates=["date"], index_col="date")

# ğŸ“Œ NaN ê°’ì´ ìˆëŠ”ì§€ í™•ì¸
print("ğŸ” ë°ì´í„° ê²°ì¸¡ì¹˜ í™•ì¸:\n", data.isna().sum())

# ğŸ“Œ ì‹œê³„ì—´ ë°ì´í„°ì…‹ ìƒì„±
def create_sequences(data, target, seq_length=12):
    X, y = [], []
    for i in range(len(data) - seq_length):
        X.append(data[i:i+seq_length])
        y.append(target[i+seq_length])
    return np.array(X), np.array(y)

# ğŸ¯ ì…ë ¥ ë° íƒ€ê²Ÿ ì„¤ì •
SEQ_LENGTH = 12
X, y = create_sequences(data.values, data["GDP"].values, SEQ_LENGTH)

# ğŸ“Œ NaN ë˜ëŠ” Inf ê°’ ì œê±°
X = np.nan_to_num(X, nan=0.0, posinf=1.0, neginf=-1.0)
y = np.nan_to_num(y, nan=0.0, posinf=1.0, neginf=-1.0)

# ğŸ“Œ ë°ì´í„°ì…‹ ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)

# ğŸ“Œ LSTM ëª¨ë¸ êµ¬ì¶• (í•™ìŠµ ì•ˆì •í™”)
print("ğŸ”§ Building LSTM model...")
model = tf.keras.models.Sequential([
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True, input_shape=(SEQ_LENGTH, X.shape[2]), recurrent_dropout=0.2)),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True, recurrent_dropout=0.2)),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=False, recurrent_dropout=0.2)),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(1, activation='linear', kernel_regularizer=tf.keras.regularizers.l2(0.01))
])

# ğŸ“Œ Adam Optimizer (í•™ìŠµë¥  ê°ì†Œ)
optimizer = tf.keras.optimizers.Adam(learning_rate=0.0005)
model.compile(optimizer=optimizer, loss='mse')

# ğŸ“Œ Early Stopping ì¶”ê°€
early_stopping = tf.keras.callbacks.EarlyStopping(
    monitor="val_loss",
    patience=15,  # ë” ì˜¤ë˜ ê¸°ë‹¤ë¦¬ê¸°
    restore_best_weights=True
)

# ğŸ“Œ ëª¨ë¸ í•™ìŠµ
print("ğŸš€ Training model...")
history = model.fit(
    X_train, y_train,
    epochs=200, batch_size=32,
    validation_data=(X_test, y_test),
    callbacks=[early_stopping]
)

# ğŸ“Œ ëª¨ë¸ ì €ì¥
model.save("model/gdp_predictor.h5")
print("âœ… Model training complete!")
